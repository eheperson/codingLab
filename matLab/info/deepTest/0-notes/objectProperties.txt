

Matlab Deep Learning Toolbox esnek network objeleri olusturmamiza 
ve bunlari sim, train, init gibi fonksiyonlar ile kullanmamiza olanak verir.
Arac kutusunun bu esnekliginin sebebi aglari nesne tabanli gosterime sahip olmasidir.


help nnnetwork          : Ag yaratma araclari

net = network           : net isimli bos bir network yaratir
                          tamamen kullanici tanimli bos bir ag yaratmak icin bu yontem kullanilabilir

########## Neural Network Section :
net.name = 'eheetwork'
net.userdata = (our user data)

######## Dimensions Section : 
> It stores the overall structure of the network.

net.numInputs = 2       : numInputs ile input kaynagi sayisi belirlenir(input vektorundeki eleman sayisi degil)
net.numLayers = 3
net.numOutputs : (2) readonly property

net.numInputDelays = 0
net.numLayerDelays = 0
net.numFeedbackDelays = 0
net.numWeightElements = 10
net.sampleTime = 1
 

######## Connections Section :
> Stores the connections between components of the network

%  biasConnect   : [0; 0; 0]
%  inputConnect  : [0 0; 0 0; 0 0]
%  layerConnect  : [0 0 0; 0 0 0; 0 0 0]
%  outputConnect : [0 0 0]

net.biasConnect(1) = 1;
net.bisConnect(3) = 1;
% net.biasConnect = [1; 0; 1]

net.inputConnect(1,1) = 1
net.inputConnect(2,1) = 1
net.inputConnect(2,2) = 1
% net.inputConnect = [1 0; 1 1; 0 0]

net.layerConnect = [0 0 0; 0 0 0; 1 1 1]

net.outputConnect = [0 1 1]

########## Subobjects Section :

    subobjects :
        inputs        : {2x1 cell array of 2 inputs}
        layers        : {3x1 cell array of 3 layers}
        outputs       : {1x3 cell array of 3 outputs}
        biases        : {3x1 cell array of 2 biases}
        inputWeight   : {3x2 cell array of 3 weights}
        layerWeughts  : {3x3 cell array of 3 weights}



######## Functions Section :

######## Weight and Bias Values Section :

######## Methods Sections :

net.inputs{i}.size 
num.inputs{1}.exampleInput = [1 1 1; 0 3 10]
                           = range
                           = size
                           = processedSize
                           = processedRange

net.layers{i}
net.layers{1}.size = 4
net.layers{1}.transferFcn = 'tansig'
net.layers{1}.initFcn = 'initnw'
net.layers{2}.size = 3
net.layers{2}.transferFcn = 'logsig'
net.layers{2}.initFcn = 'initnw'
net.layers{3}.initFcn = 'initnw'



net.outputs
net.outputs{i}.size          : the size is automatically set to 3
net.outputs{i}.exampleOutput : automatically causes size, range, processedSize processedRange to be updated
net.outputs{i}.processFcns   : Automatically causes : processParams, processSettings, processeRange

!!! Outputs have processing properties that are automatically applied to target values 
   before they are used by the network during training



net.biases
net.inputWeights
net.layerWeights

net.biases{i}
net.inputWeights{i,j}.delays = [0 1]
net.inputWeights{i,j}.delays = 1
net.layerWeights{i,j}.delays = 1

> net
    functions :
net.initFcn = 'initlay'
    initlay : network initializes itself according to the layer initialization
              functions (already set to 'initaw' > Nygin-Widraw
net.performFcnn = 'mse'
net.trainFcn = 'trainlm'
net.divideFcn = 'divideRand'
net.plotFcbs = {'plotperform', plottrainstate}

!!! During supervised training,  the input and target data are randomly divided into 
    training, test and validation datasets. The network is trained on the training data 
    until its performance begins to decrease on the validation, which signals that
    generalization has peaked.
    The test data provides a completely independent test of network generalization.

!!! Each input weight net.IW{i,j}, layer weight net.LW{i,j} and bias vector net.b{i}
    has a many rows as the size of the ith layer net.layers{i}.size.

!!! Each net.IW{i,j}, has as many columns as the size of the jth input net.inputs{i}.size 
    multiplied by the number of its delay values length(net.inputWights{i,i}.delays)

!!! Each net.LW{i,j} has as many columns as the size of the jth layer net.layers{j}.size
    multipliedby the number of its delay values length(net.layerWeights{i,j}.delays)


> It is the subobject of the network objects

net.layers{i}
net.layerWeights{i,j}



nrt.inputs
net.inputs{i}

net.inputs{i}.exampleInput = [0 10 5; 0 3 10]  
> If we set this parameter range,size processedSize, processedRange
  properties with auto br updated.

help nn.process

net.inputs{i}.processFcns = {'removeconstantrows', 'mapminmax'} 
> processParams, processSettings, processedRange and processedSize have all been updated to reflect that
  inputs will be processed using 'removeconstantrows' and 'mapminmax' before being given to the network
  when the network is simulated or trained.

ner.inouts{i}.size = 5
> We can set the size of an input directly when no processing functions are used.

processParams : contains the default parameters for each processing functions we can alterthe values, if we like.















    